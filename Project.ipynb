{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import torchvision\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_dir = './RCdata/rating_final.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(rating_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "rating_train, rating_test = train_test_split(ratings, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = len(rating_train.userID.unique())\n",
    "n_items = len(rating_train.placeID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users\n",
    "n_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = np.array(ratings.userID.unique())\n",
    "items = np.array(ratings.placeID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use ratings = food + service rating put together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_mat = np.zeros((n_users, n_items))\n",
    "food_mat = np.zeros((n_users, n_items))\n",
    "service_mat = np.zeros((n_users, n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings_mat = ratings_mat.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ratings_mat.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U1077'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings['userID'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ratings)):\n",
    "    user, = np.where(users == ratings['userID'].iloc[i])\n",
    "    item, = np.where(items == ratings['placeID'].iloc[i])\n",
    "    ratings_mat[user[0]][item[0]] = ratings['food_rating'].iloc[i] + ratings['service_rating'].iloc[i]\n",
    "    #food_mat[user[0]][item[0]] = ratings['food_rating'].iloc[i]\n",
    "    #service_mat[user[0]][item[0]] = ratings['service_rating'].iloc[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import rand as sprand\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_train = lil_matrix((n_users, n_items), dtype = float)\n",
    "for row in rating_train.itertuples():\n",
    "    user, = np.where(users == row[1])\n",
    "    item, = np.where(items == row[2])\n",
    "    interactions_train[user[0], item[0]] = row[4] + row[5]                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_test = lil_matrix((n_users, n_items), dtype = float)\n",
    "for row in rating_test.itertuples():\n",
    "    user, = np.where(users == row[1])\n",
    "    item, = np.where(items == row[2])\n",
    "    interactions_test[user[0], item[0]] = row[4] + row[5] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<138x130 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 271 stored elements in LInked List format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions\n",
    "interactions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only use the ratings right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items, n_factors=5):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users,\n",
    "                                               n_factors,\n",
    "                                               sparse=False)\n",
    "        self.item_factors = torch.nn.Embedding(n_items,n_factors,sparse=False)\n",
    "                                               \n",
    "\n",
    "    # For convenience when we want to predict a sinble user-item pair.\n",
    "    def predict(self, user, item):\n",
    "        # Need to fit bias factors\n",
    "        return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
    "    \n",
    "    # Much more efficient batch operator. This should be used for training purposes\n",
    "    \n",
    "    def forward(self, users, items):\n",
    "        #return (self.user_factors(user) * self.item_factors(item)).sum(1)\n",
    "    \n",
    "        return torch.mm(self.user_factors(users),torch.transpose(self.item_factors(items),0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasedMatrixFactorization(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_users, n_items,r_mean,n_factors=5):\n",
    "        super().__init__()\n",
    "        self.user_factors = torch.nn.Embedding(n_users,\n",
    "                                               n_factors,\n",
    "                                               sparse=False)\n",
    "        self.item_factors = torch.nn.Embedding(n_items, n_factors,sparse=False)\n",
    "        self.user_biases = torch.nn.Embedding(n_users, 1, sparse = False)\n",
    "        \n",
    "        self.item_biases = torch.nn.Embedding(n_items,1, sparse = False)\n",
    "                                               \n",
    "        self.mu = r_mean\n",
    "    \n",
    "    \n",
    "    def forward(self, users, items, values):\n",
    "        \n",
    "        item_means = []\n",
    "        for i in range(values.shape[0]):\n",
    "            item_means.append(torch.t(self.item_biases(items)))\n",
    "        \n",
    "        it_means = torch.cat(item_means, 0)\n",
    "        \n",
    "        orig = torch.mm(self.user_factors(users),torch.transpose(self.item_factors(items),0,1))\n",
    "        \n",
    "        y = torch.add(orig, self.mu)\n",
    "        \n",
    "        result = torch.add(y,it_means)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size,ratings):\n",
    "    # Sort our data and scramble it\n",
    "    rows, cols = ratings.shape\n",
    "    p = np.random.permutation(rows)\n",
    "    \n",
    "    # create batches\n",
    "    sindex = 0\n",
    "    eindex = batch_size\n",
    "    while eindex < rows:\n",
    "        batch = p[sindex:eindex]\n",
    "        temp = eindex\n",
    "        eindex = eindex + batch_size\n",
    "        sindex = temp\n",
    "        yield batch\n",
    "\n",
    "    if eindex >= rows:\n",
    "        batch = range(sindex,rows)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model,load_path):\n",
    "    load_dict = torch.load(load_path)\n",
    "    val_loss = load_dict['val_loss']\n",
    "    model.load_state_dict(load_dict['model_state_dict'])\n",
    "\n",
    "\n",
    "def checkpoint_model(val_loss, model,save_path):\n",
    "    save_dict = dict(\n",
    "                     val_loss=val_loss,\n",
    "                     model_state_dict=model.state_dict())\n",
    "                     #opt_state_dict=predictions.state_dict())\n",
    "    torch.save(save_dict, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validation(model, test_ratings, BATCH_SIZE, load_path):\n",
    "    load_model(model, load_path)\n",
    "    running_loss = 0.0\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    \n",
    "    for i,batch in enumerate(get_batch(BATCH_SIZE, test_ratings)):\n",
    "        \n",
    "        interactions = Variable(torch.FloatTensor(test_ratings[batch, :].toarray()))\n",
    "        rows = Variable(torch.LongTensor(batch))\n",
    "        cols = Variable(torch.LongTensor(np.arange(test_ratings.shape[1])))\n",
    "        \n",
    "        predictions = model(rows, cols)\n",
    "        loss = loss_func(predictions, interactions)\n",
    "        \n",
    "        running_loss += np.sqrt(loss.data[0])*BATCH_SIZE\n",
    "    \n",
    "    epoch_loss = running_loss/test_ratings.shape[0]\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "save_path = './project_models/model.ckpt'\n",
    "save_path_val = './project_models/modelv.ckpt'\n",
    "factors = [2,3,4,5]\n",
    "decays = [0.1,0.01,0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FACTOR: 2\n",
      "Decay: 0.1\n",
      "train_loss: 1.481248646747751\n",
      "checkpoint model with train loss: 1.481248646747751\n",
      "train_loss: 1.4558739628088015\n",
      "checkpoint model with train loss: 1.4558739628088015\n",
      "train_loss: 1.4688140668314547\n",
      "train_loss: 1.484083306214848\n",
      "train_loss: 1.4788010628372157\n",
      "train_loss: 1.4653422505484925\n",
      "train_loss: 1.4623684803879804\n",
      "train_loss: 1.4754434826910003\n",
      "train_loss: 1.4604227178223828\n",
      "train_loss: 1.4663995829639238\n",
      "val loss: 1.4262151263914435\n",
      "checkpointing_model with val loss: 1.4262151263914435\n",
      "Decay: 0.01\n",
      "train_loss: 1.5300230985641003\n",
      "checkpoint model with train loss: 1.5300230985641003\n",
      "train_loss: 1.530252410192068\n",
      "train_loss: 1.527851069590733\n",
      "checkpoint model with train loss: 1.527851069590733\n",
      "train_loss: 1.5403772133239515\n",
      "train_loss: 1.5520643303326842\n",
      "train_loss: 1.5525928107012832\n",
      "train_loss: 1.5274293360756583\n",
      "checkpoint model with train loss: 1.5274293360756583\n",
      "train_loss: 1.5468949919699964\n",
      "train_loss: 1.4888168297928122\n",
      "checkpoint model with train loss: 1.4888168297928122\n",
      "train_loss: 1.5368240931198942\n",
      "val loss: 1.4461602263508044\n",
      "Decay: 0.001\n",
      "train_loss: 1.544960643768257\n",
      "checkpoint model with train loss: 1.544960643768257\n",
      "train_loss: 1.558861378771102\n",
      "train_loss: 1.5346487604594798\n",
      "checkpoint model with train loss: 1.5346487604594798\n",
      "train_loss: 1.5700517033771828\n",
      "train_loss: 1.5488127781072554\n",
      "train_loss: 1.546127280825431\n",
      "train_loss: 1.5641712552772609\n",
      "train_loss: 1.5381349946894893\n",
      "train_loss: 1.5355649687698758\n",
      "train_loss: 1.5220026773676198\n",
      "checkpoint model with train loss: 1.5220026773676198\n",
      "val loss: 1.448613859710315\n",
      "FACTOR: 3\n",
      "Decay: 0.1\n",
      "train_loss: 1.7805660905619365\n",
      "checkpoint model with train loss: 1.7805660905619365\n",
      "train_loss: 1.792732067569302\n",
      "train_loss: 1.7652576839506386\n",
      "checkpoint model with train loss: 1.7652576839506386\n",
      "train_loss: 1.7857284977739527\n",
      "train_loss: 1.7460261533866488\n",
      "checkpoint model with train loss: 1.7460261533866488\n",
      "train_loss: 1.7823516334580183\n",
      "train_loss: 1.771080806328186\n",
      "train_loss: 1.760174883116953\n",
      "train_loss: 1.7863104172784574\n",
      "train_loss: 1.7367297718319772\n",
      "checkpoint model with train loss: 1.7367297718319772\n",
      "val loss: 1.6980839415841968\n",
      "Decay: 0.01\n",
      "train_loss: 1.9148357221765722\n",
      "checkpoint model with train loss: 1.9148357221765722\n",
      "train_loss: 1.9003599626853516\n",
      "checkpoint model with train loss: 1.9003599626853516\n",
      "train_loss: 1.9108818202857782\n",
      "train_loss: 1.902617282896531\n",
      "train_loss: 1.891288762759666\n",
      "checkpoint model with train loss: 1.891288762759666\n",
      "train_loss: 1.9070015883584572\n",
      "train_loss: 1.8555827404164325\n",
      "checkpoint model with train loss: 1.8555827404164325\n",
      "train_loss: 1.900882572921373\n",
      "train_loss: 1.8721878292289056\n",
      "train_loss: 1.8927496019675458\n",
      "val loss: 1.8617456109128552\n",
      "Decay: 0.001\n",
      "train_loss: 1.8677644322889595\n",
      "checkpoint model with train loss: 1.8677644322889595\n",
      "train_loss: 1.8692386490850337\n",
      "train_loss: 1.8739961570018404\n",
      "train_loss: 1.8807940287559415\n",
      "train_loss: 1.8422518608546021\n",
      "checkpoint model with train loss: 1.8422518608546021\n",
      "train_loss: 1.8519530700929565\n",
      "train_loss: 1.8657815713131307\n",
      "train_loss: 1.8352309873319508\n",
      "checkpoint model with train loss: 1.8352309873319508\n",
      "train_loss: 1.8597569480406755\n",
      "train_loss: 1.8704128856450897\n",
      "val loss: 1.7975053235332856\n",
      "FACTOR: 4\n",
      "Decay: 0.1\n",
      "train_loss: 2.158217738975398\n",
      "checkpoint model with train loss: 2.158217738975398\n",
      "train_loss: 2.1578147899415563\n",
      "checkpoint model with train loss: 2.1578147899415563\n",
      "train_loss: 2.1466967070395326\n",
      "checkpoint model with train loss: 2.1466967070395326\n",
      "train_loss: 2.16499401913718\n",
      "train_loss: 2.1314797487731454\n",
      "checkpoint model with train loss: 2.1314797487731454\n",
      "train_loss: 2.157751143156786\n",
      "train_loss: 2.141820533702753\n",
      "train_loss: 2.1283932361160596\n",
      "checkpoint model with train loss: 2.1283932361160596\n",
      "train_loss: 2.1265400729255823\n",
      "checkpoint model with train loss: 2.1265400729255823\n",
      "train_loss: 2.139877347790894\n",
      "val loss: 2.0972059026200616\n",
      "Decay: 0.01\n",
      "train_loss: 1.9580842842668917\n",
      "checkpoint model with train loss: 1.9580842842668917\n",
      "train_loss: 1.9874289084014813\n",
      "train_loss: 1.977063188926167\n",
      "train_loss: 1.9518717024459116\n",
      "checkpoint model with train loss: 1.9518717024459116\n",
      "train_loss: 1.943365394373317\n",
      "checkpoint model with train loss: 1.943365394373317\n",
      "train_loss: 1.9510411127701317\n",
      "train_loss: 1.9622412099587336\n",
      "train_loss: 1.9554489309524323\n",
      "train_loss: 1.9289191316341066\n",
      "checkpoint model with train loss: 1.9289191316341066\n",
      "train_loss: 1.9450686813650813\n",
      "val loss: 1.9078213798826722\n",
      "Decay: 0.001\n",
      "train_loss: 2.117362287449068\n",
      "checkpoint model with train loss: 2.117362287449068\n",
      "train_loss: 2.1199296537311825\n",
      "train_loss: 2.122976474249982\n",
      "train_loss: 2.1235949888578625\n",
      "train_loss: 2.0810891476763946\n",
      "checkpoint model with train loss: 2.0810891476763946\n",
      "train_loss: 2.1150020426961977\n",
      "train_loss: 2.116894170096571\n",
      "train_loss: 2.1130449075585975\n",
      "train_loss: 2.0948325446642557\n",
      "train_loss: 2.119014884060379\n",
      "val loss: 2.053303501859102\n",
      "FACTOR: 5\n",
      "Decay: 0.1\n",
      "train_loss: 2.266009478454171\n",
      "checkpoint model with train loss: 2.266009478454171\n",
      "train_loss: 2.229819336709169\n",
      "checkpoint model with train loss: 2.229819336709169\n",
      "train_loss: 2.2667220739175122\n",
      "train_loss: 2.2545865962287475\n",
      "train_loss: 2.2386328330458225\n",
      "train_loss: 2.221474438162991\n",
      "checkpoint model with train loss: 2.221474438162991\n",
      "train_loss: 2.245713505926786\n",
      "train_loss: 2.2626951690608843\n",
      "train_loss: 2.234326196152467\n",
      "train_loss: 2.210750388338851\n",
      "checkpoint model with train loss: 2.210750388338851\n",
      "val loss: 2.1868131906458714\n",
      "Decay: 0.01\n",
      "train_loss: 2.280955653895769\n",
      "checkpoint model with train loss: 2.280955653895769\n",
      "train_loss: 2.2945694666858856\n",
      "train_loss: 2.289141055357444\n",
      "train_loss: 2.294541969529947\n",
      "train_loss: 2.291800335635739\n",
      "train_loss: 2.2507707085849153\n",
      "checkpoint model with train loss: 2.2507707085849153\n",
      "train_loss: 2.2881557599505205\n",
      "train_loss: 2.2763872474387754\n",
      "train_loss: 2.2731155007684216\n",
      "train_loss: 2.2657693513153143\n",
      "val loss: 2.21482985223129\n",
      "Decay: 0.001\n",
      "train_loss: 2.4376392533536175\n",
      "checkpoint model with train loss: 2.4376392533536175\n",
      "train_loss: 2.4159001393785227\n",
      "checkpoint model with train loss: 2.4159001393785227\n",
      "train_loss: 2.419220517591237\n",
      "train_loss: 2.3928766456223385\n",
      "checkpoint model with train loss: 2.3928766456223385\n",
      "train_loss: 2.407435187163642\n",
      "train_loss: 2.4111161954046025\n",
      "train_loss: 2.410068723129047\n",
      "train_loss: 2.3668699431861313\n",
      "checkpoint model with train loss: 2.3668699431861313\n",
      "train_loss: 2.39784396919918\n",
      "train_loss: 2.4085158809885763\n",
      "val loss: 2.3593728150076814\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = 1000.0\n",
    "for f in range(len(factors)):\n",
    "    print (\"FACTOR: \" + str(factors[f]))\n",
    "    for r in range(len(decays)):\n",
    "        print (\"Decay: \" + str(decays[r]))\n",
    "        #best_val_loss = 1000.0\n",
    "        best_train_loss = 1000.0\n",
    "        model = MatrixFactorization(interactions_train.shape[0], interactions_train.shape[1], n_factors=factors[f])\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "        reg_loss_func = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=decays[r])\n",
    "        for i in range(10):\n",
    "            running_loss = 0.0\n",
    "            for m, batch in enumerate(get_batch(batch_size, interactions_train)):\n",
    "                reg_loss_func.zero_grad()\n",
    "                interactions = Variable(torch.FloatTensor(interactions_train[batch,:].toarray()))\n",
    "                rows = Variable(torch.LongTensor(batch))\n",
    "                cols = Variable(torch.LongTensor(np.arange(interactions_train.shape[1])))\n",
    "                predictions = model(rows, cols)\n",
    "        \n",
    "                loss = loss_func(predictions, interactions)\n",
    "                running_loss += np.sqrt(loss.data[0])*batch_size\n",
    "        \n",
    "                loss.backward()\n",
    "            reg_loss_func.step()\n",
    "    \n",
    "            epoch_loss = running_loss/interactions_train.shape[0]\n",
    "            print ('train_loss: ' + str(epoch_loss))\n",
    "            if epoch_loss < best_train_loss:\n",
    "                best_train_loss = epoch_loss\n",
    "                print ('checkpoint model with train loss: ' + str(best_train_loss))\n",
    "                checkpoint_model(epoch_loss, model, save_path)\n",
    "        val_loss = run_validation(model, interactions_test, batch_size, save_path)\n",
    "        print ('val loss: ' + str(val_loss))\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print ('checkpointing_model with val loss: ' + str(best_val_loss))\n",
    "            checkpoint_model(val_loss, model, save_path_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
